{"name":"Linear-algebra","tagline":"Linear algebra for Nim","body":"#Linear Algebra for Nim\r\n\r\nThis library is meant to provide basic linear algebra operations for Nim\r\napplications. The ambition would be to become a stable basis on which to\r\ndevelop a scientific ecosystem for Nim, much like Numpy does for Python.\r\n\r\nThe library has been tested on Ubuntu Linux 14.10 through 15.10 64-bit using\r\neither ATLAS, OpenBlas or Intel MKL. It was also tested on OSX Yosemite. The\r\nGPU support has been tested using NVIDIA CUDA 7.0 and 7.5.\r\n\r\nAPI documentation is [here](http://unicredit.github.io/linear-algebra/api.html)\r\n\r\nA lot of examples are available in the tests.\r\n\r\nTable of contents\r\n-----------------\r\n<!-- TOC depthFrom:2 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->\r\n\r\n- [Introduction](#introduction)\r\n- [Initialization](#initialization)\r\n- [Accessors](#accessors)\r\n- [Iterators](#iterators)\r\n- [Equality](#equality)\r\n- [Pretty-print](#pretty-print)\r\n- [Operations](#operations)\r\n- [Trivial operations](#trivial-operations)\r\n- [Universal functions](#universal-functions)\r\n- [Rewrite rules](#rewrite-rules)\r\n- [Type safety guarantees](#type-safety-guarantees)\r\n- [Linking BLAS implementations](#linking-blas-implementations)\r\n- [GPU support](#gpu-support)\r\n- [TODO](#todo)\r\n- [Contributing](#contributing)\r\n\r\n<!-- /TOC -->\r\n\r\n##Introduction\r\n\r\nThe library revolves around operations on vectors and matrices of floating\r\npoint numbers. It allows to compute operations either on the CPU or on the\r\nGPU offering identical APIs. Also, whenever possible, the dimension of vectors\r\nand matrices are encoded into the types in the form of `static[int]` type\r\nparameters. This allow to check dimensions at compile time and refuse to\r\ncompile invalid operations, such as summing two vectors of different sizes,\r\nor multiplying two matrices of incompatible dimensions.\r\n\r\nThe library defines types `Matrix64[M, N]` and `Vector64[N]` for 64-bit matrices\r\nand vectors of static dimension, as well as their 32-bit counterparts\r\n`Matrix32[M, N]` and `Vector32[N]`.\r\n\r\nFor the case where the dimension is not known at compile time, one can use\r\nso-called *dynamic* vectors and matrices, whose types are `DVector64` and\r\n`DMatrix64` (resp. `DVector32` and `DMatrix32`). Note that `DVector64` is just\r\nand alias for `seq[float64]` (and similarly for 32-bit), which allows to\r\nperform linear algebra operations on standard Nim sequences.\r\n\r\nIn all examples, types are inferred, and are shown just for explanatory purposes.\r\n\r\n##Initialization\r\n\r\nHere we show a few ways to create matrices and vectors. All matrices methods\r\naccept a parameter to define whether to store the matrix in row-major (that is,\r\ndata are laid out in memory row by row) or column-major order (that is, data\r\nare laid out in memory column by column). The default is in each case\r\ncolumn-major.\r\n\r\nWhenever possible, we try to deduce whether to use 32 or 64 bits by appropriate\r\nparameters. When this is not possible, there is an optional parameter `float32`\r\nthat can be passed to specify the precision (the default is 64 bit).\r\n\r\nStatic matrices and vectors can be created like this:\r\n\r\n```nim\r\nimport linalg\r\n\r\nlet\r\n  v1: Vector64[5] = makeVector(5, proc(i: int): float64 = (i * i).float64)\r\n  v2: Vector64[7] = randomVector(7, max = 3.0) # max is optional, default 1\r\n  v3: Vector64[5] = constantVector(5, 3.5)\r\n  v4: Vector64[8] = zeros(8)\r\n  v5: Vector64[9] = ones(9)\r\n  v6: Vector64[5] = vector([1.0, 2.0, 3.0, 4.0, 5.0]) # initialize from an array...\r\n  m1: Matrix64[6, 3] = makeMatrix(6, 3, proc(i, j: int): float64 = (i + j).float64)\r\n  m2: Matrix64[2, 8] = randomMatrix(2, 8, max = 1.6) # max is optional, default 1\r\n  m3: Matrix64[3, 5] = constantMatrix(3, 5, 1.8, order = rowMajor) # order is optional, default colMajor\r\n  m4: Matrix64[3, 6] = ones(3, 6)\r\n  m5: Matrix64[5, 2] = zeros(5, 2)\r\n  m6: Matrix64[7, 7] = eye(7)\r\n  m7: Matrix64[2, 3] = matrix([\r\n    [1.2, 3.5, 4.3],\r\n    [1.1, 4.2, 1.7]\r\n  ])\r\n  m8: Matrix64[2, 3] = matrix(@[\r\n    @[1.2, 3.5, 4.3],\r\n    @[1.1, 4.2, 1.7]\r\n  ], 2, 3)\r\n```\r\n\r\nThe last `matrix` constructor takes a `seq` of `seq`s, but also requires\r\nstatically passing the dimensions to be used. The following are equivalent\r\nwhen `xs` is a `seq[seq[float64]]` and `M`, `N` are integers known at compile\r\ntime:\r\n\r\n```nim\r\nlet\r\n  m1 = matrix(xs).toStatic(M, N)\r\n  m2 = matrix(xs, M, N)\r\n```\r\n\r\nbut the latter form avoids the construction of an intermediate matrix.\r\n\r\nAll constructors that take as input an existing array or seq perform a copy of\r\nthe data for memory safety.\r\n\r\nDynamic matrices and vectors have similar constructors - the difference is that\r\nthe dimension parameter are not known at compile time:\r\n\r\n```nim\r\nimport linalg\r\n\r\nlet\r\n  M = 5\r\n  N = 7\r\n  v1: DVector64 = makeVector(M, proc(i: int): float64 = (i * i).float64)\r\n  v2: DVector64 = randomVector(N, max = 3.0) # max is optional, default 1\r\n  v3: DVector64 = constantVector(M, 3.5)\r\n  v4: DVector64 = zeros(M)\r\n  v5: DVector64 = ones(N)\r\n  v6: DVector64 = @[1.0, 2.0, 3.0, 4.0, 5.0] # DVectors are just seqs...\r\n  m1: DMatrix64 = makeMatrix(M, N, proc(i, j: int): float64 = (i + j).float64)\r\n  m2: DMatrix64 = randomMatrix(M, N, max = 1.6) # max is optional, default 1\r\n  m3: DMatrix64 = constantMatrix(M, N, 1.8, order = rowMajor) # order is optional, default colMajor\r\n  m4: DMatrix64 = ones(M, N)\r\n  m5: DMatrix64 = zeros(M, N)\r\n  m6: DMatrix64 = eye(M)\r\n  m7: DMatrix64 = matrix(@[\r\n    @[1.2, 3.5, 4.3],\r\n    @[1.1, 4.2, 1.7]\r\n  ])\r\n```\r\n\r\nIf for some reason you want to create a dynamic vector of matrix, but you want\r\nto write literal dimensions, you can either assign these numbers to variables\r\nor use the `toDynamic` proc to convert the static instances to dynamic ones:\r\n\r\n```nim\r\nimport linalg\r\n\r\nlet\r\n  M = 5\r\n  v1 = makeVector(M, proc(i: int): float64 = (i * i).float64)\r\n  v2 = makeVector(5, proc(i: int): float64 = (i * i).float64)\r\n\r\nassert v1.toStatic(5) == v2\r\nassert v2.toDynamic == v1\r\n```\r\n\r\nWorking with 32-bit\r\n-------------------\r\n\r\nOne can also instantiate 32-bit matrices and vectors. Whenever possible, the\r\nAPI is identical. In cases that would be ambiguous (such as `zeros`), one can\r\nexplicitly specify the `float32` parameter.\r\n\r\n```nim\r\nimport linalg\r\n\r\nlet\r\n  v1: Vector32[5] = makeVector(5, proc(i: int): float32 = (i * i).float32)\r\n  v2: Vector32[7] = randomVector(7, max = 3'f32) # max is no longer optional, to distinguish 32/64 bit\r\n  v3: Vector32[5] = constantVector(5, 3.5'f32)\r\n  v4: Vector32[8] = zeros(8, float32)\r\n  v5: Vector32[9] = ones(9, float32)\r\n  v6: Vector32[5] = vector([1'f32, 2'f32, 3'f32, 4'f32, 5'f32])\r\n  m1: Matrix32[6, 3] = makeMatrix(6, 3, proc(i, j: int): float32 = (i + j).float32)\r\n  m2: Matrix32[2, 8] = randomMatrix(2, 8, max = 1.6'f32)\r\n  m3: Matrix32[3, 5] = constantMatrix(3, 5, 1.8'f32, order = rowMajor) # order is optional, default colMajor\r\n  m4: Matrix32[3, 6] = ones(3, 6, float32)\r\n  m5: Matrix32[5, 2] = zeros(5, 2, float32)\r\n  m6: Matrix32[7, 7] = eye(7, float32)\r\n  m7: Matrix32[2, 3] = matrix([\r\n    [1.2'f32, 3.5'f32, 4.3'f32],\r\n    [1.1'f32, 4.2'f32, 1.7'f32]\r\n  ])\r\n  m8: Matrix32[2, 3] = matrix(@[\r\n    @[1.2'f32, 3.5'f32, 4.3'f32],\r\n    @[1.1'f32, 4.2'f32, 1.7'f32]\r\n  ], 2, 3)\r\n```\r\n\r\nSimilarly,\r\n\r\n```nim\r\nimport linalg\r\n\r\nlet\r\n  M = 5\r\n  N = 7\r\n  v1: DVector32 = makeVector(M, proc(i: int): float32 = (i * i).float32)\r\n  v2: DVector32 = randomVector(N, max = 3'f32) # max is not optional\r\n  v3: DVector32 = constantVector(M, 3.5'f32)\r\n  v4: DVector32 = zeros(M, float32)\r\n  v5: DVector32 = ones(N, float32)\r\n  v6: DVector32 = @[1'f32, 2'f32, 3'f32, 4'f32, 5'f32] # DVectors are just seqs...\r\n  m1: DMatrix32 = makeMatrix(M, N, proc(i, j: int): float32 = (i + j).float32)\r\n  m2: DMatrix32 = randomMatrix(M, N, max = 1.6'f32) # max is not optional\r\n  m3: DMatrix32 = constantMatrix(M, N, 1.8'f32, order = rowMajor) # order is optional, default colMajor\r\n  m4: DMatrix32 = ones(M, N, float32)\r\n  m5: DMatrix32 = zeros(M, N, float32)\r\n  m6: DMatrix32 = eye(M, float32)\r\n  m7: DMatrix32 = matrix(@[\r\n    @[1.2'f32, 3.5'f32, 4.3'f32],\r\n    @[1.1'f32, 4.2'f32, 1.7'f32]\r\n  ])\r\n```\r\n\r\nOne can convert precision with `to32` or `to64`:\r\n\r\n```nim\r\nlet\r\n  v64: Vector64[10] = randomVector(10)\r\n  v32: Vector32[10] = v64.to32()\r\n  m32: Matrix32[3, 8] = randomMatrix(3, 8, max = 1'f32)\r\n  m64: Matrix64[3, 8] = m32.to64()\r\n```\r\n\r\nOnce vectors and matrices are created, everything is inferred, so there are no\r\ndifferences in working with 32-bit or 64-bit. All examples that follow are for\r\n64-bit, but they would work as well for 32-bit.\r\n\r\n##Accessors\r\n\r\nVectors can be accessed as expected:\r\n\r\n```nim\r\nvar v = randomVector(6)\r\nv[4] = 1.2\r\necho v[3]\r\n```\r\n\r\nSame for matrices, where `m[i, j]` denotes the item on row `i` and column `j`,\r\nregardless of the matrix order:\r\n\r\n```nim\r\nvar m = randomMatrix(3, 7)\r\nm[1, 3] = 0.8\r\necho m[2, 2]\r\n```\r\n\r\nAlso one can see rows and columns as vectors\r\n\r\n```nim\r\nlet\r\n  r2: Vector64[7] = m.row(2)\r\n  c5: Vector64[3] = m.column(5)\r\n```\r\n\r\nFor memory safety, this performs a **copy** of the row or column values, at\r\nleast for now. One can also map vectors and matrices via a proc:\r\n\r\n```nim\r\nlet\r\n  v1 = v.map(proc(x: float64): float64 = 2 - 3 * x)\r\n  m1 = m.map(proc(x: float64): float64 = 1 / x)\r\n```\r\n\r\nSimilar operations are available for dynamic matrices and vectors as well.\r\n\r\n##Iterators\r\n\r\nOne can iterate over vector or matrix elements, as well as over rows and columns\r\n\r\n```nim\r\nlet\r\n  v = randomVector(6)\r\n  m = randomMatrix(3, 5)\r\nfor x in v: echo x\r\nfor i, x in v: echo i, x\r\nfor x in m: echo x\r\nfor t, x in m:\r\n  let (i, j) = t\r\n  echo i, j, x\r\nfor row in m.rows:\r\n  echo row[0]\r\nfor column in m.columns:\r\n  echo column[1]\r\n```\r\n\r\n##Equality\r\n\r\nThere are two kinds of equality. The usual `==` operator will compare the\r\ncontents of vector and matrices exactly\r\n\r\n```nim\r\nlet\r\n  u = vector([1.0, 2.0, 3.0, 4.0])\r\n  v = vector([1.0, 2.0, 3.0, 4.0])\r\n  w = vector([1.0, 3.0, 3.0, 4.0])\r\nu == v # true\r\nu == w # false\r\n```\r\n\r\nUsually, though, one wants to take into account the errors introduced by\r\nfloating point operations. To do this, use the `=~` operator, or its\r\nnegation `!=~`:\r\n\r\n```nim\r\nlet\r\n  u = vector([1.0, 2.0, 3.0, 4.0])\r\n  v = vector([1.0, 2.000000001, 2.99999999, 4.0])\r\nu == v # false\r\nu =~ v # true\r\n```\r\n\r\n##Pretty-print\r\n\r\nBoth vectors and matrix have a pretty-print operation, so one can do\r\n\r\n```nim\r\nlet m = randomMatrix(3, 7)\r\necho m8\r\n```\r\n\r\nand get something like\r\n\r\n    [ [ 0.5024584865674662  0.0798945419892334  0.7512423051567048  0.9119041361916302  0.5868388894943912  0.3600554448403415  0.4419034543022882 ]\r\n      [ 0.8225964245706265  0.01608615513584155 0.1442007939324697  0.7623388321096165  0.8419745686508193  0.08792951865247645 0.2902529012579151 ]\r\n      [ 0.8488187232786935  0.422866666087792 0.1057975175658363  0.07968277822379832 0.7526946339452074  0.7698915909784674  0.02831893268471575 ] ]\r\n\r\n##Operations\r\n\r\nA few linear algebra operations are available, wrapping BLAS libraries:\r\n\r\n```nim\r\nvar v1 = randomVector(7)\r\nlet\r\n  v2 = randomVector(7)\r\n  m1 = randomMatrix(6, 9)\r\n  m2 = randomMatrix(9, 7)\r\necho 3.5 * v1\r\nv1 *= 2.3\r\necho v1 + v2\r\necho v1 - v2\r\necho v1 * v2 # dot product\r\necho l_1(v1) # l_1 norm\r\necho l_2(v1) # l_2 norm\r\necho m2 * v1 # matrix-vector product\r\necho m1 * m2 # matrix-matrix product\r\necho max(m1)\r\necho min(v2)\r\n```\r\n\r\n##Trivial operations\r\n\r\nThe following operations do not change the underlying memory layout of matrices\r\nand vectors. This means they run in very little time even on big matrices, but\r\nyou have to pay attention when mutating matrices and vectors produced in this\r\nway, since the underlying data is shared.\r\n\r\n```nim\r\nlet\r\n  m1 = randomMatrix(6, 9)\r\n  m2 = randomMatrix(9, 6)\r\n  v1 = randomVector(9)\r\necho m1.t # transpose, done in constant time without copying\r\necho m1 + m2.t\r\nlet m3: Matrix64[9, 6] = m1.reshape(9, 6)\r\nlet m4: Matrix64[3, 3] = v1.asMatrix(3, 3)\r\nlet v2: Vector64[54] = m2.asVector\r\n```\r\n\r\nIn case you need to allocate a copy of the original data, say in order to\r\ntranspose a matrix and then mutate the transpose without altering the original\r\nmatrix, a `clone` operation is available:\r\n\r\n```nim\r\nlet m5 = m1.clone\r\n```\r\n\r\n##Universal functions\r\n\r\nUniversal functions are real-valued functions that are extended to vectors\r\nand matrices by working element-wise. There are many common functions that are\r\nimplemented as universal functions:\r\n\r\n```nim\r\nsqrt\r\ncbrt\r\nlog10\r\nlog2\r\nlog\r\nexp\r\narccos\r\narcsin\r\narctan\r\ncos\r\ncosh\r\nsin\r\nsinh\r\ntan\r\ntanh\r\nerf\r\nerfc\r\nlgamma\r\ntgamma\r\ntrunc\r\nfloor\r\nceil\r\ndegToRad\r\nradToDeg\r\n```\r\n\r\nThis means that, for instance, the following check passes:\r\n\r\n```nim\r\n  let\r\n    v1 = vector([1.0, 2.3, 4.5, 3.2, 5.4])\r\n    v2 = log(v1)\r\n    v3 = v1.map(log)\r\n\r\n  assert v2 == v3\r\n```\r\n\r\nUniversal functions work both on 32 and 64 bit precision, on vectors and\r\nmatrices, both static and dynamic.\r\n\r\nIf you have a function `f` of type `proc(x: float64): float64` you can use\r\n\r\n```nim\r\nmakeUniversal(f)\r\n```\r\n\r\nto turn `f` into a (public) universal function. If you do not want to export\r\n`f`, there is the equivalent template `makeUniversalLocal`.\r\n\r\n##Rewrite rules\r\n\r\nA few rewrite rules allow to optimize a chain of linear algebra operations\r\ninto a single BLAS call. For instance, if you try\r\n\r\n```nim\r\necho v1 + 5.3 * v2\r\n```\r\n\r\nthis is not implemented as a scalar multiplication followed by a sum, but it\r\nis turned into a single function call.\r\n\r\n##Type safety guarantees\r\n\r\nThe library is designed with the use case of having dimensions known at compile\r\ntime, and leverages the compiles to ensure that dimensions match when performing\r\nthe appropriate operations - for instance in matrix multiplication.\r\n\r\nTo see some examples where the compiler avoids malformed operations, look\r\ninside `tests/compilation` (yes, in Nim one can actually test that some\r\noperations do not compile!).\r\n\r\nAlso, operations that mutate a vector of matrix in place are only available if\r\nthat vector of matrix is defined via `var` instead of `let`.\r\n\r\n##Linking BLAS implementations\r\n\r\nThe library requires to link some BLAS implementation to perform the actual\r\nlinear algebra operations. By default, it tries to link whatever is the default\r\nsystem-wide BLAS implementation.\r\n\r\nA few compile flags are available to link specific BLAS implementations\r\n\r\n    -d:atlas\r\n    -d:openblas\r\n    -d:mkl\r\n    -d:mkl -d:threaded\r\n\r\nPackages for various BLAS implementations are available from the package\r\nmanagers of many Linux distributions. On OSX one can add the brew formulas\r\nfrom [Homebrew Science](https://github.com/Homebrew/homebrew-science), such\r\nas `brew install homebrew/science/openblas`.\r\n\r\nYou may also need to add suitable paths for the includes and library dirs.\r\nOn OSX, this should do the trick\r\n\r\n```nim\r\nswitch(\"clibdir\", \"/usr/local/opt/openblas/lib\")\r\nswitch(\"cincludes\", \"/usr/local/opt/openblas/include\")\r\n```\r\n\r\n##GPU support\r\n\r\nIt is possible to delegate work to the GPU using CUDA. The library has been\r\ntested to work with NVIDIA CUDA 7.0 and 7.5, but it is possible that earlier\r\nversions will work as well. In order to compile and link against CUDA, you\r\nshould make the appropriate headers and libraries available. If they are not\r\nglobally set, you can pass suitable options to the Nim compiler, such as\r\n\r\n    --cincludes:\"/usr/local/cuda/targets/x86_64-linux/include\" \\\r\n    --clibdir:\"/usr/local/cuda/targets/x86_64-linux/lib\"\r\n\r\nYou will also need to explicitly add `linalg` support for CUDA with the flag\r\n\r\n    -d:cublas\r\n\r\nSupport is currently limited to 32-bit operations on static matrices and\r\nvectors, which is the most common case, but 64-bit and dynamic instances will\r\nalso be implemented soon.\r\n\r\nIf you have a 32-bit matrix or vector, you can move it on the GPU, and back\r\nlike this\r\n\r\n```nim\r\nlet\r\n  v: Vector32[12] = randomVector(12, max=1'f32)\r\n  vOnTheGpu: CudaVector[12] = v.gpu()\r\n  vBackOnTheCpu: Vector32[12] = vOnTheGpu.cpu()\r\n```\r\n\r\nVectors and matrices on the GPU support linear-algebraic operations via cuBLAS,\r\nexactly like their CPU counterparts. A few operation - such as reading a single\r\nelement - are not supported, as it does not make much sense to copy a single\r\nvalue back and forth from the GPU. Usually it is advisable to move vectors\r\nand matrices to the GPU, make as man computations as possible there, and\r\nfinally move the result back to the CPU.\r\n\r\nThe following are all valid operations, assuming `v` and `w` are vectors on the\r\nGPU, `m` and `n` are matrices on the GPU and the dimensions are compatible:\r\n\r\n```nim\r\nv * 3'f32\r\nv + w\r\nv -= w\r\nm * v\r\nm - n\r\nm * n\r\n```\r\n\r\nFor more information, look at the tests in `tests/cuda`.\r\n\r\n##TODO\r\n\r\n* Add support for matrices and vector on the stack\r\n* Add more functional interfaces (foldl, scanl)\r\n* Add more common operations (cumsum, mean, stdv...)\r\n* Use rewrite rules to optimize complex operations into a single BLAS call\r\n* 64-bit and dynamic GPU support\r\n* More specialized BLAS operations\r\n* Add operations from LAPACK\r\n* Support slicing/nonconstant steps\r\n* Make `row` and `column` operations non-copying\r\n* Better types to avoid out of bounds exceptions when statically checkable\r\n* Add a fallback Nim implementation, that is valid over other rings\r\n* Move BLAS and CUBLAS to separate libraries required via nimble\r\n* Try on more platforms/configurations\r\n* Make a proper benchmark\r\n* Improve documentation\r\n* Better pretty-print\r\n\r\n##Contributing\r\n\r\nEvery contribution is very much appreciated! This can range from:\r\n\r\n* using the library and reporting any issues and any configuration on which\r\n  it works fine\r\n* building other parts of the scientific environment on top of it\r\n* writing blog posts and tutorials\r\n* contributing actual code (see the **TODO** section)\r\n\r\nThe library has to cater many different use cases, hence the vector and matrix\r\ntypes differ in various axes:\r\n\r\n* 32/64 bit\r\n* CPU/GPU\r\n* static/dynamic\r\n* (on the stack? non-contiguous? non GC pointers?)\r\n\r\nIn order to avoid a combinatorial explosion of operations, a judicious use of\r\ntemplates and union types helps to limit the actual implementations that have\r\nto be written.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}