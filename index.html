<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Linear-algebra by unicredit</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Linear-algebra</h1>
      <h2 class="project-tagline">Linear algebra for Nim</h2>
      <a href="https://github.com/unicredit/linear-algebra" class="btn">View on GitHub</a>
      <a href="https://github.com/unicredit/linear-algebra/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/unicredit/linear-algebra/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="linear-algebra-for-nim" class="anchor" href="#linear-algebra-for-nim" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear Algebra for Nim</h1>

<p>This library is meant to provide basic linear algebra operations for Nim
applications. The ambition would be to become a stable basis on which to
develop a scientific ecosystem for Nim, much like Numpy does for Python.</p>

<p>The library has been tested on Ubuntu Linux 14.10 through 15.10 64-bit using
either ATLAS, OpenBlas or Intel MKL. The GPU support has been tested using
NVIDIA CUDA 7.0 and 7.5.</p>

<p>API documentation is <a href="http://unicredit.github.io/linear-algebra/api.html">here</a></p>

<h2>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>

<p>The library revolves around operations on vectors and matrices of floating
point numbers. It allows to compute operations either on the CPU or on the
GPU offering identical APIs. Also, whenever possible, the dimension of vectors
and matrices are encoded into the types in the form of <code>static[int]</code> type
parameters. This allow to check dimensions at compile time and refuse to
compile invalid operations, such as summing two vectors of different sizes,
or multiplying two matrices of incompatible dimensions.</p>

<p>The library defines types <code>Matrix64[M, N]</code> and <code>Vector64[N]</code> for 64-bit matrices
and vectors of static dimension, as well as their 32-bit counterparts
<code>Matrix32[M, N]</code> and <code>Vector32[N]</code>.</p>

<p>For the case where the dimension is not known at compile time, one can use
so-called <em>dynamic</em> vectors and matrices, whose types are <code>DVector64</code> and
<code>DMatrix64</code> (resp. <code>DVector32</code> and <code>DMatrix32</code>). Note that <code>DVector64</code> is just
and alias for <code>seq[float64]</code> (and similarly for 32-bit), which allows to
perform linear algebra operations on standard Nim sequences.</p>

<p>In all examples, types are inferred, and are shown just for explanatory purposes.</p>

<h2>
<a id="initialization" class="anchor" href="#initialization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Initialization</h2>

<p>Here we show a few ways to create matrices and vectors. All matrices methods
accept a parameter to define whether to store the matrix in row-major (that is,
data are laid out in memory row by row) or column-major order (that is, data
are laid out in memory column by column). The default is in each case
column-major.</p>

<p>Whenever possible, we try to deduce whether to use 32 or 64 bits by appropriate
parameters. When this is not possible, there is an optional parameter <code>float32</code>
that can be passed to specify the precision (the default is 64 bit).</p>

<p>Static matrices and vectors can be created like this:</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">import</span> linalg

<span class="pl-k">let</span>
  v1: Vector64[<span class="pl-c1">5</span>] = <span class="pl-c1">makeVector</span>(<span class="pl-c1">5</span>, <span class="pl-k">proc</span>(i: <span class="pl-k">int</span>)<span class="pl-k">:</span> <span class="pl-k">float64</span> = (i * i).<span class="pl-k">float64</span>)
  v2: Vector64[<span class="pl-c1">7</span>] = <span class="pl-c1">randomVector</span>(<span class="pl-c1">7</span>, max = <span class="pl-c1">3.0</span>) <span class="pl-c"># max is optional, default 1</span>
  v3: Vector64[<span class="pl-c1">5</span>] = <span class="pl-c1">constantVector</span>(<span class="pl-c1">5</span>, <span class="pl-c1">3.5</span>)
  v4: Vector64[<span class="pl-c1">8</span>] = <span class="pl-c1">zeros</span>(<span class="pl-c1">8</span>)
  v5: Vector64[<span class="pl-c1">9</span>] = <span class="pl-c1">ones</span>(<span class="pl-c1">9</span>)
  v6: Vector64[<span class="pl-c1">5</span>] = <span class="pl-c1">vector</span>([<span class="pl-c1">1.0</span>, <span class="pl-c1">2.0</span>, <span class="pl-c1">3.0</span>, <span class="pl-c1">4.0</span>, <span class="pl-c1">5.0</span>]) <span class="pl-c"># initialize from an array...</span>
  m1: Matrix64[<span class="pl-c1">6</span>, <span class="pl-c1">3</span>] = <span class="pl-c1">makeMatrix</span>(<span class="pl-c1">6</span>, <span class="pl-c1">3</span>, <span class="pl-k">proc</span>(i, j: <span class="pl-k">int</span>)<span class="pl-k">:</span> <span class="pl-k">float64</span> = (i + j).<span class="pl-k">float64</span>)
  m2: Matrix64[<span class="pl-c1">2</span>, <span class="pl-c1">8</span>] = <span class="pl-c1">randomMatrix</span>(<span class="pl-c1">2</span>, <span class="pl-c1">8</span>, max = <span class="pl-c1">1.6</span>) <span class="pl-c"># max is optional, default 1</span>
  m3: Matrix64[<span class="pl-c1">3</span>, <span class="pl-c1">5</span>] = <span class="pl-c1">constantMatrix</span>(<span class="pl-c1">3</span>, <span class="pl-c1">5</span>, <span class="pl-c1">1.8</span>, order = rowMajor) <span class="pl-c"># order is optional, default colMajor</span>
  m4: Matrix64[<span class="pl-c1">3</span>, <span class="pl-c1">6</span>] = <span class="pl-c1">ones</span>(<span class="pl-c1">3</span>, <span class="pl-c1">6</span>)
  m5: Matrix64[<span class="pl-c1">5</span>, <span class="pl-c1">2</span>] = <span class="pl-c1">zeros</span>(<span class="pl-c1">5</span>, <span class="pl-c1">2</span>)
  m6: Matrix64[<span class="pl-c1">7</span>, <span class="pl-c1">7</span>] = <span class="pl-c1">eye</span>(<span class="pl-c1">7</span>)
  m7: Matrix64[<span class="pl-c1">2</span>, <span class="pl-c1">3</span>] = <span class="pl-c1">matrix</span>([
    [<span class="pl-c1">1.2</span>, <span class="pl-c1">3.5</span>, <span class="pl-c1">4.3</span>],
    [<span class="pl-c1">1.1</span>, <span class="pl-c1">4.2</span>, <span class="pl-c1">1.7</span>]
  ])
  m8: Matrix64[<span class="pl-c1">2</span>, <span class="pl-c1">3</span>] = <span class="pl-c1">matrix</span>(@[
    <span class="pl-k">@</span>[<span class="pl-c1">1.2</span>, <span class="pl-c1">3.5</span>, <span class="pl-c1">4.3</span>],
    <span class="pl-k">@</span>[<span class="pl-c1">1.1</span>, <span class="pl-c1">4.2</span>, <span class="pl-c1">1.7</span>]
  ], <span class="pl-c1">2</span>, <span class="pl-c1">3</span>)</pre></div>

<p>The last <code>matrix</code> constructor takes a <code>seq</code> of <code>seq</code>s, but also requires
statically passing the dimensions to be used. The following are equivalent
when <code>xs</code> is a <code>seq[seq[float64]]</code> and <code>M</code>, <code>N</code> are integers known at compile
time:</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">let</span>
  m1 = <span class="pl-c1">matrix</span>(xs)<span class="pl-k">.</span><span class="pl-c1">toStatic</span>(M, N)
  m2 = <span class="pl-c1">matrix</span>(xs, M, N)</pre></div>

<p>but the latter form avoids the construction of an intermediate matrix.</p>

<p>All constructors that take as input an existing array or seq perform a copy of
the data for memory safety.</p>

<p>Dynamic matrices and vectors have similar constructors - the difference is that
the dimension parameter are not known at compile time:</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">import</span> linalg

<span class="pl-k">let</span>
  M = <span class="pl-c1">5</span>
  N = <span class="pl-c1">7</span>
  v1: DVector64 = <span class="pl-c1">makeVector</span>(M, <span class="pl-k">proc</span>(i: <span class="pl-k">int</span>)<span class="pl-k">:</span> <span class="pl-k">float64</span> = (i * i).<span class="pl-k">float64</span>)
  v2: DVector64 = <span class="pl-c1">randomVector</span>(N, max = <span class="pl-c1">3.0</span>) <span class="pl-c"># max is optional, default 1</span>
  v3: DVector64 = <span class="pl-c1">constantVector</span>(M, <span class="pl-c1">3.5</span>)
  v4: DVector64 = <span class="pl-c1">zeros</span>(M)
  v5: DVector64 = <span class="pl-c1">ones</span>(N)
  v6: DVector64 = @[<span class="pl-c1">1.0</span>, <span class="pl-c1">2.0</span>, <span class="pl-c1">3.0</span>, <span class="pl-c1">4.0</span>, <span class="pl-c1">5.0</span>] <span class="pl-c"># DVectors are just seqs...</span>
  m1: DMatrix64 = <span class="pl-c1">makeMatrix</span>(M, N, <span class="pl-k">proc</span>(i, j: <span class="pl-k">int</span>)<span class="pl-k">:</span> <span class="pl-k">float64</span> = (i + j).<span class="pl-k">float64</span>)
  m2: DMatrix64 = <span class="pl-c1">randomMatrix</span>(M, N, max = <span class="pl-c1">1.6</span>) <span class="pl-c"># max is optional, default 1</span>
  m3: DMatrix64 = <span class="pl-c1">constantMatrix</span>(M, N, <span class="pl-c1">1.8</span>, order = rowMajor) <span class="pl-c"># order is optional, default colMajor</span>
  m4: DMatrix64 = <span class="pl-c1">ones</span>(M, N)
  m5: DMatrix64 = <span class="pl-c1">zeros</span>(M, N)
  m6: DMatrix64 = <span class="pl-c1">eye</span>(M)
  m7: DMatrix64 = <span class="pl-c1">matrix</span>(@[
    <span class="pl-k">@</span>[<span class="pl-c1">1.2</span>, <span class="pl-c1">3.5</span>, <span class="pl-c1">4.3</span>],
    <span class="pl-k">@</span>[<span class="pl-c1">1.1</span>, <span class="pl-c1">4.2</span>, <span class="pl-c1">1.7</span>]
  ])</pre></div>

<p>If for some reason you want to create a dynamic vector of matrix, but you want
to write literal dimensions, you can either assign these numbers to variables
or use the <code>toDynamic</code> proc to convert the static instances to dynamic ones:</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">import</span> linalg

<span class="pl-k">let</span>
  M = <span class="pl-c1">5</span>
  v1 = <span class="pl-c1">makeVector</span>(M, <span class="pl-k">proc</span>(i: <span class="pl-k">int</span>)<span class="pl-k">:</span> <span class="pl-k">float64</span> = (i * i).<span class="pl-k">float64</span>)
  v2 = <span class="pl-c1">makeVector</span>(<span class="pl-c1">5</span>, <span class="pl-k">proc</span>(i: <span class="pl-k">int</span>)<span class="pl-k">:</span> <span class="pl-k">float64</span> = (i * i).<span class="pl-k">float64</span>)

<span class="pl-c1">assert</span> v1.<span class="pl-c1">toStatic</span>(<span class="pl-c1">5</span>) <span class="pl-k">==</span> v2
<span class="pl-c1">assert</span> v2.toDynamic == v1</pre></div>

<h2>
<a id="working-with-32-bit" class="anchor" href="#working-with-32-bit" aria-hidden="true"><span class="octicon octicon-link"></span></a>Working with 32-bit</h2>

<p>One can also instantiate 32-bit matrices and vectors. Whenever possible, the
API is identical. In cases that would be ambiguous (such as <code>zeros</code>), one can
explicitly specify the <code>float32</code> parameter.</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">import</span> linalg

<span class="pl-k">let</span>
  v1: Vector32[<span class="pl-c1">5</span>] = <span class="pl-c1">makeVector</span>(<span class="pl-c1">5</span>, <span class="pl-k">proc</span>(i: <span class="pl-k">int</span>)<span class="pl-k">:</span> <span class="pl-k">float32</span> = (i * i).<span class="pl-k">float32</span>)
  v2: Vector32[<span class="pl-c1">7</span>] = <span class="pl-c1">randomVector</span>(<span class="pl-c1">7</span>, max = <span class="pl-c1">3'f32</span>) <span class="pl-c"># max is no longer optional, to distinguish 32/64 bit</span>
  v3: Vector32[<span class="pl-c1">5</span>] = <span class="pl-c1">constantVector</span>(<span class="pl-c1">5</span>, <span class="pl-c1">3.5'f32</span>)
  v4: Vector32[<span class="pl-c1">8</span>] = <span class="pl-c1">zeros</span>(<span class="pl-c1">8</span>, <span class="pl-k">float32</span>)
  v5: Vector32[<span class="pl-c1">9</span>] = <span class="pl-c1">ones</span>(<span class="pl-c1">9</span>, <span class="pl-k">float32</span>)
  v6: Vector32[<span class="pl-c1">5</span>] = <span class="pl-c1">vector</span>([<span class="pl-c1">1'f32</span>, <span class="pl-c1">2'f32</span>, <span class="pl-c1">3'f32</span>, <span class="pl-c1">4'f32</span>, <span class="pl-c1">5'f32</span>])
  m1: Matrix32[<span class="pl-c1">6</span>, <span class="pl-c1">3</span>] = <span class="pl-c1">makeMatrix</span>(<span class="pl-c1">6</span>, <span class="pl-c1">3</span>, <span class="pl-k">proc</span>(i, j: <span class="pl-k">int</span>)<span class="pl-k">:</span> <span class="pl-k">float32</span> = (i + j).<span class="pl-k">float32</span>)
  m2: Matrix32[<span class="pl-c1">2</span>, <span class="pl-c1">8</span>] = <span class="pl-c1">randomMatrix</span>(<span class="pl-c1">2</span>, <span class="pl-c1">8</span>, max = <span class="pl-c1">1.6'f32</span>)
  m3: Matrix32[<span class="pl-c1">3</span>, <span class="pl-c1">5</span>] = <span class="pl-c1">constantMatrix</span>(<span class="pl-c1">3</span>, <span class="pl-c1">5</span>, <span class="pl-c1">1.8'f32</span>, order = rowMajor) <span class="pl-c"># order is optional, default colMajor</span>
  m4: Matrix32[<span class="pl-c1">3</span>, <span class="pl-c1">6</span>] = <span class="pl-c1">ones</span>(<span class="pl-c1">3</span>, <span class="pl-c1">6</span>, <span class="pl-k">float32</span>)
  m5: Matrix32[<span class="pl-c1">5</span>, <span class="pl-c1">2</span>] = <span class="pl-c1">zeros</span>(<span class="pl-c1">5</span>, <span class="pl-c1">2</span>, <span class="pl-k">float32</span>)
  m6: Matrix32[<span class="pl-c1">7</span>, <span class="pl-c1">7</span>] = <span class="pl-c1">eye</span>(<span class="pl-c1">7</span>, <span class="pl-k">float32</span>)
  m7: Matrix32[<span class="pl-c1">2</span>, <span class="pl-c1">3</span>] = <span class="pl-c1">matrix</span>([
    [<span class="pl-c1">1.2'f32</span>, <span class="pl-c1">3.5'f32</span>, <span class="pl-c1">4.3'f32</span>],
    [<span class="pl-c1">1.1'f32</span>, <span class="pl-c1">4.2'f32</span>, <span class="pl-c1">1.7'f32</span>]
  ])
  m8: Matrix32[<span class="pl-c1">2</span>, <span class="pl-c1">3</span>] = <span class="pl-c1">matrix</span>(@[
    <span class="pl-k">@</span>[<span class="pl-c1">1.2'f32</span>, <span class="pl-c1">3.5'f32</span>, <span class="pl-c1">4.3'f32</span>],
    <span class="pl-k">@</span>[<span class="pl-c1">1.1'f32</span>, <span class="pl-c1">4.2'f32</span>, <span class="pl-c1">1.7'f32</span>]
  ], <span class="pl-c1">2</span>, <span class="pl-c1">3</span>)</pre></div>

<p>Similarly,</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">import</span> linalg

<span class="pl-k">let</span>
  M = <span class="pl-c1">5</span>
  N = <span class="pl-c1">7</span>
  v1: DVector32 = <span class="pl-c1">makeVector</span>(M, <span class="pl-k">proc</span>(i: <span class="pl-k">int</span>)<span class="pl-k">:</span> <span class="pl-k">float32</span> = (i * i).<span class="pl-k">float32</span>)
  v2: DVector32 = <span class="pl-c1">randomVector</span>(N, max = <span class="pl-c1">3'f32</span>) <span class="pl-c"># max is not optional</span>
  v3: DVector32 = <span class="pl-c1">constantVector</span>(M, <span class="pl-c1">3.5'f32</span>)
  v4: DVector32 = <span class="pl-c1">zeros</span>(M, <span class="pl-k">float32</span>)
  v5: DVector32 = <span class="pl-c1">ones</span>(N, <span class="pl-k">float32</span>)
  v6: DVector32 = @[<span class="pl-c1">1'f32</span>, <span class="pl-c1">2'f32</span>, <span class="pl-c1">3'f32</span>, <span class="pl-c1">4'f32</span>, <span class="pl-c1">5'f32</span>] <span class="pl-c"># DVectors are just seqs...</span>
  m1: DMatrix32 = <span class="pl-c1">makeMatrix</span>(M, N, <span class="pl-k">proc</span>(i, j: <span class="pl-k">int</span>)<span class="pl-k">:</span> <span class="pl-k">float32</span> = (i + j).<span class="pl-k">float32</span>)
  m2: DMatrix32 = <span class="pl-c1">randomMatrix</span>(M, N, max = <span class="pl-c1">1.6'f32</span>) <span class="pl-c"># max is not optional</span>
  m3: DMatrix32 = <span class="pl-c1">constantMatrix</span>(M, N, <span class="pl-c1">1.8'f32</span>, order = rowMajor) <span class="pl-c"># order is optional, default colMajor</span>
  m4: DMatrix32 = <span class="pl-c1">ones</span>(M, N, <span class="pl-k">float32</span>)
  m5: DMatrix32 = <span class="pl-c1">zeros</span>(M, N, <span class="pl-k">float32</span>)
  m6: DMatrix32 = <span class="pl-c1">eye</span>(M, <span class="pl-k">float32</span>)
  m7: DMatrix32 = <span class="pl-c1">matrix</span>(@[
    <span class="pl-k">@</span>[<span class="pl-c1">1.2'f32</span>, <span class="pl-c1">3.5'f32</span>, <span class="pl-c1">4.3'f32</span>],
    <span class="pl-k">@</span>[<span class="pl-c1">1.1'f32</span>, <span class="pl-c1">4.2'f32</span>, <span class="pl-c1">1.7'f32</span>]
  ])</pre></div>

<p>One can convert precision with <code>to32</code> or <code>to64</code>:</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">let</span>
  v64: Vector64[<span class="pl-c1">10</span>] = <span class="pl-c1">randomVector</span>(<span class="pl-c1">10</span>)
  v32: Vector32[<span class="pl-c1">10</span>] = v64.<span class="pl-c1">to32</span>()
  m32: Matrix32[<span class="pl-c1">3</span>, <span class="pl-c1">8</span>] = <span class="pl-c1">randomMatrix</span>(<span class="pl-c1">3</span>, <span class="pl-c1">8</span>, max = <span class="pl-c1">1'f32</span>)
  m64: Matrix64[<span class="pl-c1">3</span>, <span class="pl-c1">8</span>] = m32.<span class="pl-c1">to64</span>()</pre></div>

<p>Once vectors and matrices are created, everything is inferred, so there are no
differences in working with 32-bit or 64-bit. All examples that follow are for
64-bit, but they would work as well for 32-bit.</p>

<h2>
<a id="accessors" class="anchor" href="#accessors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Accessors</h2>

<p>Vectors can be accessed as expected:</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">var</span> v = <span class="pl-c1">randomVector</span>(<span class="pl-c1">6</span>)
v[<span class="pl-c1">4</span>] = <span class="pl-c1">1.2</span>
<span class="pl-c1">echo</span> v[<span class="pl-c1">3</span>]</pre></div>

<p>Same for matrices, where <code>m[i, j]</code> denotes the item on row <code>i</code> and column <code>j</code>,
regardless of the matrix order:</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">var</span> m = <span class="pl-c1">randomMatrix</span>(<span class="pl-c1">3</span>, <span class="pl-c1">7</span>)
m[<span class="pl-c1">1</span>, <span class="pl-c1">3</span>] = <span class="pl-c1">0.8</span>
<span class="pl-c1">echo</span> m[<span class="pl-c1">2</span>, <span class="pl-c1">2</span>]</pre></div>

<p>Also one can see rows and columns as vectors</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">let</span>
  r2: Vector64[<span class="pl-c1">7</span>] = m.<span class="pl-c1">row</span>(<span class="pl-c1">2</span>)
  c5: Vector64[<span class="pl-c1">3</span>] = m.<span class="pl-c1">column</span>(<span class="pl-c1">5</span>)</pre></div>

<p>For memory safety, this performs a <strong>copy</strong> of the row or column values, at
least for now. One can also map vectors and matrices via a proc:</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">let</span>
  v1 = v.<span class="pl-c1">map</span>(<span class="pl-k">proc</span>(x: <span class="pl-k">float64</span>)<span class="pl-k">:</span> <span class="pl-k">float64</span> = <span class="pl-c1">2</span> - <span class="pl-c1">3</span> * x)
  m1 = m.<span class="pl-c1">map</span>(<span class="pl-k">proc</span>(x: <span class="pl-k">float64</span>)<span class="pl-k">:</span> <span class="pl-k">float64</span> = <span class="pl-c1">1</span> / x)</pre></div>

<p>Similar operations are available for dynamic matrices and vectors as well.</p>

<h2>
<a id="iterators" class="anchor" href="#iterators" aria-hidden="true"><span class="octicon octicon-link"></span></a>Iterators</h2>

<p>One can iterate over vector or matrix elements, as well as over rows and columns</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">let</span>
  v = <span class="pl-c1">randomVector</span>(<span class="pl-c1">6</span>)
  m = <span class="pl-c1">randomMatrix</span>(<span class="pl-c1">3</span>, <span class="pl-c1">5</span>)
<span class="pl-k">for</span> x <span class="pl-k">in</span> v: <span class="pl-c1">echo</span> x
<span class="pl-k">for</span> i, x <span class="pl-k">in</span> v: <span class="pl-c1">echo</span> i, x
<span class="pl-k">for</span> x <span class="pl-k">in</span> m: <span class="pl-c1">echo</span> x
<span class="pl-k">for</span> t, x <span class="pl-k">in</span> m:
  <span class="pl-k">let</span> (i, j) = t
  <span class="pl-c1">echo</span> i, j, x
<span class="pl-k">for</span> row <span class="pl-k">in</span> m.rows:
  <span class="pl-c1">echo</span> row[<span class="pl-c1">0</span>]
<span class="pl-k">for</span> column <span class="pl-k">in</span> m.columns:
  <span class="pl-c1">echo</span> column[<span class="pl-c1">1</span>]</pre></div>

<h2>
<a id="equality" class="anchor" href="#equality" aria-hidden="true"><span class="octicon octicon-link"></span></a>Equality</h2>

<p>There are two kinds of equality. The usual <code>==</code> operator will compare the
contents of vector and matrices exactly</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">let</span>
  u = <span class="pl-c1">vector</span>([<span class="pl-c1">1.0</span>, <span class="pl-c1">2.0</span>, <span class="pl-c1">3.0</span>, <span class="pl-c1">4.0</span>])
  v = <span class="pl-c1">vector</span>([<span class="pl-c1">1.0</span>, <span class="pl-c1">2.0</span>, <span class="pl-c1">3.0</span>, <span class="pl-c1">4.0</span>])
  w = <span class="pl-c1">vector</span>([<span class="pl-c1">1.0</span>, <span class="pl-c1">3.0</span>, <span class="pl-c1">3.0</span>, <span class="pl-c1">4.0</span>])
u == v <span class="pl-c"># true</span>
u == w <span class="pl-c"># false</span></pre></div>

<p>Usually, though, one wants to take into account the errors introduced by
floating point operations. To do this, use the <code>=~</code> operator, or its
negation <code>!=~</code>:</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">let</span>
  u = <span class="pl-c1">vector</span>([<span class="pl-c1">1.0</span>, <span class="pl-c1">2.0</span>, <span class="pl-c1">3.0</span>, <span class="pl-c1">4.0</span>])
  v = <span class="pl-c1">vector</span>([<span class="pl-c1">1.0</span>, <span class="pl-c1">2.000000001</span>, <span class="pl-c1">2.99999999</span>, <span class="pl-c1">4.0</span>])
u == v <span class="pl-c"># false</span>
u =~ v <span class="pl-c"># true</span></pre></div>

<h2>
<a id="pretty-print" class="anchor" href="#pretty-print" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pretty-print</h2>

<p>Both vectors and matrix have a pretty-print operation, so one can do</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">let</span> m = <span class="pl-c1">randomMatrix</span>(<span class="pl-c1">3</span>, <span class="pl-c1">7</span>)
<span class="pl-c1">echo</span> m8</pre></div>

<p>and get something like</p>

<pre><code>[ [ 0.5024584865674662  0.0798945419892334  0.7512423051567048  0.9119041361916302  0.5868388894943912  0.3600554448403415  0.4419034543022882 ]
  [ 0.8225964245706265  0.01608615513584155 0.1442007939324697  0.7623388321096165  0.8419745686508193  0.08792951865247645 0.2902529012579151 ]
  [ 0.8488187232786935  0.422866666087792 0.1057975175658363  0.07968277822379832 0.7526946339452074  0.7698915909784674  0.02831893268471575 ] ]
</code></pre>

<h2>
<a id="operations" class="anchor" href="#operations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Operations</h2>

<p>A few linear algebra operations are available, wrapping BLAS libraries:</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">var</span> v1 = <span class="pl-c1">randomVector</span>(<span class="pl-c1">7</span>)
<span class="pl-k">let</span>
  v2 = <span class="pl-c1">randomVector</span>(<span class="pl-c1">7</span>)
  m1 = <span class="pl-c1">randomMatrix</span>(<span class="pl-c1">6</span>, <span class="pl-c1">9</span>)
  m2 = <span class="pl-c1">randomMatrix</span>(<span class="pl-c1">9</span>, <span class="pl-c1">7</span>)
<span class="pl-c1">echo</span> <span class="pl-c1">3.5</span> * v1
v1 *= <span class="pl-c1">2.3</span>
echo v1 + v2
echo v1 - v2
echo v1 * v2 <span class="pl-c"># dot product</span>
<span class="pl-c1">echo</span> <span class="pl-c1">l_1</span>(v1) <span class="pl-c"># l_1 norm</span>
<span class="pl-c1">echo</span> <span class="pl-c1">l_2</span>(v1) <span class="pl-c"># l_2 norm</span>
echo m2 * v1 <span class="pl-c"># matrix-vector product</span>
echo m1 * m2 <span class="pl-c"># matrix-matrix product</span>
<span class="pl-c1">echo</span> <span class="pl-c1">max</span>(m1)
<span class="pl-c1">echo</span> <span class="pl-c1">min</span>(v2)</pre></div>

<h2>
<a id="trivial-operations" class="anchor" href="#trivial-operations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Trivial operations</h2>

<p>The following operations do not change the underlying memory layout of matrices
and vectors. This means they run in very little time even on big matrices, but
you have to pay attention when mutating matrices and vectors produced in this
way, since the underlying data is shared.</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">let</span>
  m1 = <span class="pl-c1">randomMatrix</span>(<span class="pl-c1">6</span>, <span class="pl-c1">9</span>)
  m2 = <span class="pl-c1">randomMatrix</span>(<span class="pl-c1">9</span>, <span class="pl-c1">6</span>)
  v1 = <span class="pl-c1">randomVector</span>(<span class="pl-c1">9</span>)
<span class="pl-c1">echo</span> m1.t <span class="pl-c"># transpose, done in constant time without copying</span>
echo m1 + m2.t
<span class="pl-k">let</span> m3: Matrix64[<span class="pl-c1">9</span>, <span class="pl-c1">6</span>] = m1.<span class="pl-c1">reshape</span>(<span class="pl-c1">9</span>, <span class="pl-c1">6</span>)
<span class="pl-k">let</span> m4: Matrix64[<span class="pl-c1">3</span>, <span class="pl-c1">3</span>] = v1.<span class="pl-c1">asMatrix</span>(<span class="pl-c1">3</span>, <span class="pl-c1">3</span>)
<span class="pl-k">let</span> v2: Vector64[<span class="pl-c1">54</span>] = m2.asVector</pre></div>

<p>In case you need to allocate a copy of the original data, say in order to
transpose a matrix and then mutate the transpose without altering the original
matrix, a <code>clone</code> operation is available:</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">let</span> m5 = m1.clone</pre></div>

<h2>
<a id="universal-functions" class="anchor" href="#universal-functions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Universal functions</h2>

<p>Universal functions are real-valued functions that are extended to vectors
and matrices by working element-wise. There are many common functions that are
implemented as universal functions:</p>

<div class="highlight highlight-source-nim"><pre>sqrt
cbrt
log10
log2
log
exp
arccos
arcsin
arctan
cos
cosh
sin
sinh
tan
tanh
erf
erfc
lgamma
tgamma
trunc
floor
ceil
degToRad
radToDeg</pre></div>

<p>This means that, for instance, the following check passes:</p>

<div class="highlight highlight-source-nim"><pre>  <span class="pl-k">let</span>
    v1 = <span class="pl-c1">vector</span>([<span class="pl-c1">1.0</span>, <span class="pl-c1">2.3</span>, <span class="pl-c1">4.5</span>, <span class="pl-c1">3.2</span>, <span class="pl-c1">5.4</span>])
    v2 = <span class="pl-c1">log</span>(v1)
    v3 = v1.<span class="pl-c1">map</span>(log)

  assert v2 == v3</pre></div>

<p>Universal functions work both on 32 and 64 bit precision, on vectors and
matrices, both static and dynamic.</p>

<p>If you have a function <code>f</code> of type <code>proc(x: float64): float64</code> you can use</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-c1">makeUniversal</span>(f)</pre></div>

<p>to turn <code>f</code> into a (public) universal function. If you do not want to export
<code>f</code>, there is the equivalent template <code>makeUniversalLocal</code>.</p>

<h2>
<a id="rewrite-rules" class="anchor" href="#rewrite-rules" aria-hidden="true"><span class="octicon octicon-link"></span></a>Rewrite rules</h2>

<p>A few rewrite rules allow to optimize a chain of linear algebra operations
into a single BLAS call. For instance, if you try</p>

<div class="highlight highlight-source-nim"><pre>echo v1 + <span class="pl-c1">5.3</span> * v2</pre></div>

<p>this is not implemented as a scalar multiplication followed by a sum, but it
is turned into a single function call.</p>

<h2>
<a id="type-safety-guarantees" class="anchor" href="#type-safety-guarantees" aria-hidden="true"><span class="octicon octicon-link"></span></a>Type safety guarantees</h2>

<p>The library is designed with the use case of having dimensions known at compile
time, and leverages the compiles to ensure that dimensions match when performing
the appropriate operations - for instance in matrix multiplication.</p>

<p>To see some examples where the compiler avoids malformed operations, look
inside <code>tests/compilation</code> (yes, in Nim one can actually test that some
operations do not compile!).</p>

<p>Also, operations that mutate a vector of matrix in place are only available if
that vector of matrix is defined via <code>var</code> instead of <code>let</code>.</p>

<h2>
<a id="linking-blas-implementations" class="anchor" href="#linking-blas-implementations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linking BLAS implementations</h2>

<p>The library requires to link some BLAS implementation to perform the actual
linear algebra operations. By default, it tries to link whatever is the default
system-wide BLAS implementation.</p>

<p>A few compile flags are available to link specific BLAS implementations</p>

<pre><code>-d:atlas
-d:openblas
-d:mkl
-d:mkl -d:threaded
</code></pre>

<h2>
<a id="gpu-support" class="anchor" href="#gpu-support" aria-hidden="true"><span class="octicon octicon-link"></span></a>GPU support</h2>

<p>It is possible to delegate work to the GPU using CUDA. The library has been
tested to work with NVIDIA CUDA 7.0 and 7.5, but it is possible that earlier
versions will work as well. In order to compile and link against CUDA, you
should make the appropriate headers and libraries available. If they are not
globally set, you can pass suitable options to the Nim compiler, such as</p>

<pre><code>--cincludes:"/usr/local/cuda/targets/x86_64-linux/include" \
--clibdir:"/usr/local/cuda/targets/x86_64-linux/lib"
</code></pre>

<p>You will also need to explicitly add <code>linalg</code> support for CUDA with the flag</p>

<pre><code>-d:cublas
</code></pre>

<p>Support is currently limited to 32-bit operations on static matrices and
vectors, which is the most common case, but 64-bit and dynamic instances will
also be implemented soon.</p>

<p>If you have a 32-bit matrix or vector, you can move it on the GPU, and back
like this</p>

<div class="highlight highlight-source-nim"><pre><span class="pl-k">let</span>
  v: Vector32[<span class="pl-c1">12</span>] = <span class="pl-c1">randomVector</span>(<span class="pl-c1">12</span>, max=<span class="pl-c1">1'f32</span>)
  vOnTheGpu: CudaVector[<span class="pl-c1">12</span>] = v.<span class="pl-c1">gpu</span>()
  vBackOnTheCpu: Vector32[<span class="pl-c1">12</span>] = vOnTheGpu.<span class="pl-c1">cpu</span>()</pre></div>

<p>Vectors and matrices on the GPU support linear-algebraic operations via cuBLAS,
exactly like their CPU counterparts. A few operation - such as reading a single
element - are not supported, as it does not make much sense to copy a single
value back and forth from the GPU. Usually it is advisable to move vectors
and matrices to the GPU, make as man computations as possible there, and
finally move the result back to the CPU.</p>

<p>The following are all valid operations, assuming <code>v</code> and <code>w</code> are vectors on the
GPU, <code>m</code> and <code>n</code> are matrices on the GPU and the dimensions are compatible:</p>

<div class="highlight highlight-source-nim"><pre>v * <span class="pl-c1">3'f32</span>
v + w
v -= w
m * v
m - n
m * n</pre></div>

<p>For more information, look at the tests in <code>tests/cuda</code>.</p>

<h2>
<a id="todo" class="anchor" href="#todo" aria-hidden="true"><span class="octicon octicon-link"></span></a>TODO</h2>

<ul>
<li>Add support for matrices and vector on the stack</li>
<li>Use rewrite rules to optimize complex operations into a single BLAS call</li>
<li>64-bit and dynamic GPU support</li>
<li>More specialized BLAS operations</li>
<li>Add operations from LAPACK</li>
<li>Support slicing/nonconstant steps</li>
<li>Make <code>row</code> and <code>column</code> operations non-copying</li>
<li>Better types to avoid out of bounds exceptions when statically checkable</li>
<li>Add a fallback Nim implementation, that is valid over other rings</li>
<li>Try on more platforms/configurations</li>
<li>Make a proper benchmark</li>
<li>Improve documentation</li>
</ul>

<h2>
<a id="contributing" class="anchor" href="#contributing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Contributing</h2>

<p>Every contribution is very much appreciated! This can range from:</p>

<ul>
<li>using the library and reporting any issues and any configuration on which
it works fine</li>
<li>building other parts of the scientific environment on top of it</li>
<li>writing blog posts and tutorials</li>
<li>contributing actual code (see the <strong>TODO</strong> section)</li>
</ul>

<p>The library has to cater many different use cases, hence the vector and matrix
types differ in various axes:</p>

<ul>
<li>32/64 bit</li>
<li>CPU/GPU</li>
<li>static/dynamic</li>
<li>(on the stack? non-contiguous? non GC pointers?)</li>
</ul>

<p>In order to avoid a combinatorial explosion of operations, a judicious use of
templates and union types helps to limit the actual implementations that have
to be written.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/unicredit/linear-algebra">Linear-algebra</a> is maintained by <a href="https://github.com/unicredit">unicredit</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>


  </body>
</html>
